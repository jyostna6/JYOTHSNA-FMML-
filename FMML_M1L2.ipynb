{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyostna6/JYOTHSNA-FMML-/blob/main/FMML_M1L2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eu9VZbF01eq"
      },
      "source": [
        "# Machine Learning terms and metrics\n",
        "\n",
        "FMML Module 1, Lab 2\n",
        "\n",
        "In this lab, we will show a part of the ML pipeline by using the California Housing dataset. There are 20640 samples, each with 8 attributes like income of the block, age of the houses per district etc. The task is to predict the cost of the houses per district. We will use the scikit-learn library to load the data and perform some basic data preprocessing and model training. We will also show how to evaluate the model using some common metrics, split the data into training and testing sets, and use cross-validation to get a better estimate of the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qBvyEem0vLi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rng = np.random.default_rng(seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LpqjN991GGJ",
        "outputId": "6f379d6f-dc2c-427a-93f7-587d7b909a5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _california_housing_dataset:\n",
            "\n",
            "California Housing dataset\n",
            "--------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 20640\n",
            "\n",
            "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
            "\n",
            "    :Attribute Information:\n",
            "        - MedInc        median income in block group\n",
            "        - HouseAge      median house age in block group\n",
            "        - AveRooms      average number of rooms per household\n",
            "        - AveBedrms     average number of bedrooms per household\n",
            "        - Population    block group population\n",
            "        - AveOccup      average number of household members\n",
            "        - Latitude      block group latitude\n",
            "        - Longitude     block group longitude\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "This dataset was obtained from the StatLib repository.\n",
            "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
            "\n",
            "The target variable is the median house value for California districts,\n",
            "expressed in hundreds of thousands of dollars ($100,000).\n",
            "\n",
            "This dataset was derived from the 1990 U.S. census, using one row per census\n",
            "block group. A block group is the smallest geographical unit for which the U.S.\n",
            "Census Bureau publishes sample data (a block group typically has a population\n",
            "of 600 to 3,000 people).\n",
            "\n",
            "A household is a group of people residing within a home. Since the average\n",
            "number of rooms and bedrooms in this dataset are provided per household, these\n",
            "columns may take surprisingly large values for block groups with few households\n",
            "and many empty houses, such as vacation resorts.\n",
            "\n",
            "It can be downloaded/loaded using the\n",
            ":func:`sklearn.datasets.fetch_california_housing` function.\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
            "      Statistics and Probability Letters, 33 (1997) 291-297\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dataset = datasets.fetch_california_housing()\n",
        "# Dataset description\n",
        "print(dataset.DESCR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCe1VNftevgE"
      },
      "source": [
        "Given below are the list of target values. These correspond to the house value derived considering all the 8 input features and are continuous values. We should use regression models to predict these values but we will start with a simple classification model for the sake of simplicity. We need to just round off the values to the nearest integer and use a classification model to predict the house value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8K0ggBOevgE",
        "outputId": "f71dd070-d8a0-41e1-b595-107fec9f47e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orignal target values: [4.526 3.585 3.521 ... 0.923 0.847 0.894]\n",
            "Target values after conversion: [4 3 3 ... 0 0 0]\n",
            "Input variables shape: (20640, 8)\n",
            "Output variables shape: (20640,)\n"
          ]
        }
      ],
      "source": [
        "print(\"Orignal target values:\", dataset.target)\n",
        "\n",
        "dataset.target = dataset.target.astype(int)\n",
        "\n",
        "print(\"Target values after conversion:\", dataset.target)\n",
        "print(\"Input variables shape:\", dataset.data.shape)\n",
        "print(\"Output variables shape:\", dataset.target.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNx4174W5xRg"
      },
      "source": [
        "The simplest model to use for classification is the K-Nearest Neighbors model. We will use this model to predict the house value with a K value of 1. We will also use the accuracy metric to evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07zpydQj1hIQ"
      },
      "outputs": [],
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and a query point\n",
        "    and returns the predicted label for the query point using the nearest neighbour algorithm\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    query: numpy array of shape (d,) where d is the number of features\n",
        "\n",
        "    returns: the predicted label for the query point which is the label of the training data which is closest to the query point\n",
        "    \"\"\"\n",
        "    diff = (\n",
        "        traindata - query\n",
        "    )  # find the difference between features. Numpy automatically takes care of the size here\n",
        "    sq = diff * diff  # square the differences\n",
        "    dist = sq.sum(1)  # add up the squares\n",
        "    label = trainlabel[np.argmin(dist)]\n",
        "    return label\n",
        "\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and test data\n",
        "    and returns the predicted labels for the test data using the nearest neighbour algorithm\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    testdata: numpy array of shape (m,d) where m is the number of test samples and d is the number of features\n",
        "\n",
        "    returns: the predicted labels for the test data which is the label of the training data which is closest to each test point\n",
        "    \"\"\"\n",
        "    predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "    return predlabel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03JktkfIGaje"
      },
      "source": [
        "We will also define a 'random classifier', which randomly allots labels to each sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fogWAtjyGhAH"
      },
      "outputs": [],
      "source": [
        "def RandomClassifier(traindata, trainlabel, testdata):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and test data\n",
        "    and returns the predicted labels for the test data using the random classifier algorithm\n",
        "\n",
        "    In reality, we don't need these arguments but we are passing them to keep the function signature consistent with other classifiers\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    testdata: numpy array of shape (m,d) where m is the number of test samples and d is the number of features\n",
        "\n",
        "    returns: the predicted labels for the test data which is a random label from the training data\n",
        "    \"\"\"\n",
        "\n",
        "    classes = np.unique(trainlabel)\n",
        "    rints = rng.integers(low=0, high=len(classes), size=len(testdata))\n",
        "    predlabel = classes[rints]\n",
        "    return predlabel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hjf1KHs7fU5"
      },
      "source": [
        "We need a metric to evaluate the performance of the model. Let us define a metric 'Accuracy' to see how good our learning algorithm is. Accuracy is the ratio of the number of correctly classified samples to the total number of samples. The higher the accuracy, the better the algorithm. We will use the accuracy metric to evaluate and compate the performance of the K-Nearest Neighbors model and the random classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouuCqWU07bz-"
      },
      "outputs": [],
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "    \"\"\"\n",
        "    This function takes in the ground-truth labels and predicted labels\n",
        "    and returns the accuracy of the classifier\n",
        "\n",
        "    gtlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    predlabel: numpy array of shape (n,) where n is the number of samples\n",
        "\n",
        "    returns: the accuracy of the classifier which is the number of correct predictions divided by the total number of predictions\n",
        "    \"\"\"\n",
        "    assert len(gtlabel) == len(\n",
        "        predlabel\n",
        "    ), \"Length of the ground-truth labels and predicted labels should be the same\"\n",
        "    correct = (\n",
        "        gtlabel == predlabel\n",
        "    ).sum()  # count the number of times the groundtruth label is equal to the predicted label.\n",
        "    return correct / len(gtlabel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJFwBFa9Klw"
      },
      "source": [
        "Let us make a function to split the dataset with the desired probability. We will use this function to split the dataset into training and testing sets. We will use the training set to train the model and the testing set to evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ko0VzpSM2Tdi"
      },
      "outputs": [],
      "source": [
        "def split(data, label, percent):\n",
        "    # generate a random number for each sample\n",
        "    rnd = rng.random(len(label))\n",
        "    split1 = rnd < percent\n",
        "    split2 = rnd >= percent\n",
        "\n",
        "    split1data = data[split1, :]\n",
        "    split1label = label[split1]\n",
        "    split2data = data[split2, :]\n",
        "    split2label = label[split2]\n",
        "    return split1data, split1label, split2data, split2label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcK3LEAJ_LGC"
      },
      "source": [
        "We will reserve 20% of our dataset as the test set. We will not change this portion throughout our experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBZkHBLJ1iU-",
        "outputId": "73ffc0bd-354d-4051-f17a-c6adebdd58fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples: 4144\n",
            "Number of train samples: 16496\n",
            "Percent of test data: 20.07751937984496 %\n"
          ]
        }
      ],
      "source": [
        "testdata, testlabel, alltraindata, alltrainlabel = split(\n",
        "    dataset.data, dataset.target, 20 / 100\n",
        ")\n",
        "print(\"Number of test samples:\", len(testlabel))\n",
        "print(\"Number of train samples:\", len(alltrainlabel))\n",
        "print(\"Percent of test data:\", len(testlabel) * 100 / len(dataset.target), \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Ss0Z6IAGNV"
      },
      "source": [
        "## Experiments with splits\n",
        "\n",
        "Let us reserve some of our train data as a validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFew2iry_7W7"
      },
      "outputs": [],
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(\n",
        "    alltraindata, alltrainlabel, 75 / 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60hiu4clFN1i"
      },
      "source": [
        "What is the accuracy of our classifiers on the train dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBlZDTHUFTZx",
        "outputId": "2d0ed616-7ece-4d8f-8f8d-cb1eb48f7385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy using nearest neighbour algorithm: 100.0 %\n",
            "Training accuracy using random classifier:  16.4375808538163 %\n"
          ]
        }
      ],
      "source": [
        "trainpred = NN(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Training accuracy using nearest neighbour algorithm:\", trainAccuracy*100, \"%\")\n",
        "\n",
        "trainpred = RandomClassifier(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Training accuracy using random classifier: \", trainAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h08-9gJDtSy"
      },
      "source": [
        "For nearest neighbour, the train accuracy is always 1. The accuracy of the random classifier is close to 1/(number of classes) which is 0.1666 in our case. This is because the random classifier randomly assigns a label to each sample and the probability of assigning the correct label is 1/(number of classes). Let us predict the labels for our validation set and get the accuracy. This accuracy is a good estimate of the accuracy of our model on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h7bXoW_2H3v",
        "outputId": "ab09a4be-1b0e-4db2-ef67-a4b5f41c48fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour algorithm: 34.10852713178294 %\n",
            "Validation accuracy using random classifier: 16.884689922480618 %\n"
          ]
        }
      ],
      "source": [
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour algorithm:\", valAccuracy*100, \"%\")\n",
        "\n",
        "\n",
        "valpred = RandomClassifier(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using random classifier:\", valAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py9bLguFEjfg"
      },
      "source": [
        "Validation accuracy of nearest neighbour is considerably less than its train accuracy while the validation accuracy of random classifier is the same. However, the validation accuracy of nearest neighbour is twice that of the random classifier. Now let us try another random split and check the validation accuracy. We will see that the validation accuracy changes with the split. This is because the validation set is small and the accuracy is highly dependent on the samples in the validation set. We can get a better estimate of the accuracy by using cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujm3cyYzEntE",
        "outputId": "8e5506e9-474c-4fd6-d7a6-b14aa1b8b8d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour algorithm: 34.048257372654156 %\n"
          ]
        }
      ],
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(\n",
        "    alltraindata, alltrainlabel, 75 / 100)\n",
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour algorithm:\", valAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSOx7U83EKie"
      },
      "source": [
        "You can run the above cell multiple times to try with different random splits.\n",
        "We notice that the accuracy is different for each run, but close together.\n",
        "\n",
        "Now let us compare it with the accuracy we get on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNEZ5ToYBEDW",
        "outputId": "2487fee9-0efc-4ce9-8b76-c8fd90b0c94a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 34.91795366795367 %\n"
          ]
        }
      ],
      "source": [
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "testAccuracy = Accuracy(testlabel, testpred)\n",
        "\n",
        "print(\"Test accuracy:\", testAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3dGD531K3gH"
      },
      "source": [
        "### Try it out for yourself and answer:\n",
        "1. How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?\n",
        "2. How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "3. What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?\n",
        "\n",
        "Answer for both nearest neighbour and random classifier. You can note down the values for your experiments and plot a graph using  <a href=https://matplotlib.org/stable/gallery/lines_bars_and_markers/step_demo.html#sphx-glr-gallery-lines-bars-and-markers-step-demo-py>plt.plot<href>. Check also for extreme values for splits, like 99.9% or 0.1%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9zvdYY6evgI"
      },
      "source": [
        "> Exercise: Try to implement a 3 nearest neighbour classifier and compare the accuracy of the 1 nearest neighbour classifier and the 3 nearest neighbour classifier on the test dataset. You can use the KNeighborsClassifier class from the scikit-learn library to implement the K-Nearest Neighbors model. You can set the number of neighbors using the n_neighbors parameter. You can also use the accuracy_score function from the scikit-learn library to calculate the accuracy of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnYvkAZLQY7h"
      },
      "source": [
        "## Multiple Splits\n",
        "\n",
        "One way to get more accurate estimates for the test accuracy is by using <b>cross-validation</b>. Here, we will try a simple version, where we do multiple train/val splits and take the average of validation accuracies as the test accuracy estimation. Here is a function for doing this. Note that this function will take a long time to execute. You can reduce the number of splits to make it faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4nGCUQXBTzo"
      },
      "outputs": [],
      "source": [
        "def AverageAccuracy(alldata, alllabel, splitpercent, iterations, classifier=NN):\n",
        "    \"\"\"\n",
        "    This function takes in the data, labels, split percentage, number of iterations and classifier function\n",
        "    and returns the average accuracy of the classifier\n",
        "\n",
        "    alldata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    alllabel: numpy array of shape (n,) where n is the number of samples\n",
        "    splitpercent: float which is the percentage of data to be used for training\n",
        "    iterations: int which is the number of iterations to run the classifier\n",
        "    classifier: function which is the classifier function to be used\n",
        "\n",
        "    returns: the average accuracy of the classifier\n",
        "    \"\"\"\n",
        "    accuracy = 0\n",
        "    for ii in range(iterations):\n",
        "        traindata, trainlabel, valdata, vallabel = split(\n",
        "            alldata, alllabel, splitpercent\n",
        "        )\n",
        "        valpred = classifier(traindata, trainlabel, valdata)\n",
        "        accuracy += Accuracy(vallabel, valpred)\n",
        "    return accuracy / iterations  # average of all accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3qtNar7Bbik",
        "outputId": "5a5a53ec-beea-4f7f-cefc-79f74cceb1bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy: 33.58463539517022 %\n",
            "Test accuracy: 34.91795366795367 %\n"
          ]
        }
      ],
      "source": [
        "avg_acc = AverageAccuracy(alltraindata, alltrainlabel, 75 / 100, 10, classifier=NN)\n",
        "print(\"Average validation accuracy:\", avg_acc*100, \"%\")\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "\n",
        "print(\"Test accuracy:\", Accuracy(testlabel, testpred)*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33GIn4x5VH-d"
      },
      "source": [
        "This is a very simple way of doing cross-validation. There are many well-known algorithms for cross-validation, like k-fold cross-validation, leave-one-out etc. This will be covered in detail in a later module. For more information about cross-validation, check <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>Cross-validatioin (Wikipedia)</a>\n",
        "\n",
        "### Questions\n",
        "1. Does averaging the validation accuracy across multiple splits give more consistent results?\n",
        "Yes, averaging the validation accuracy across multiple splits can indeed provide more consistent results. This technique is commonly referred to as cross-validation and is widely used to assess the generalizability of a model. Here's why it works:\n",
        "\n",
        "Why Averaging Validation Accuracy Helps with Consistency\n",
        "\n",
        "1. Reduces Variability:\n",
        "\n",
        "A single train-test split can be highly dependent on the particular data you happen to get in the test set. If the test set is not representative of the overall dataset (due to randomness), your model evaluation may be biased.\n",
        "\n",
        "Multiple splits average out this randomness by using different subsets of the data for training and validation, leading to a more robust and reliable estimate of the model's performance.\n",
        "\n",
        "\n",
        "\n",
        "2. Better Generalization Estimate:\n",
        "\n",
        "When you perform k-fold cross-validation (typically with k=5 or k=10), you partition the data into k subsets. The model is trained on k-1 folds and tested on the remaining fold, and this process is repeated k times, each with a different test fold.\n",
        "\n",
        "Averaging the results across these splits helps to obtain a better estimate of how the model will perform on unseen data, because it has been validated on multiple, distinct subsets of the data.\n",
        "\n",
        "\n",
        "\n",
        "3. Handles Data Imbalances Better:\n",
        "\n",
        "In the case of imbalanced data (where some classes are underrepresented), a single train-test split might not reflect the performance on all classes well. Multiple splits help to ensure that each class appears in both training and validation sets in different ways, making the model's performance more consistent across different subsets.\n",
        "\n",
        "\n",
        "\n",
        "4. Mitigates Overfitting:\n",
        "\n",
        "If a model performs well on a single train-test split but poorly on others, it suggests the model might be overfitting to particular subsets of the data. Averaging across multiple validation splits helps highlight any overfitting tendencies and provides a more reliable estimate of how the model will perform on new, unseen data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Cross-Validation Types\n",
        "\n",
        "There are several types of cross-validation you can use to average the validation accuracy over multiple splits:\n",
        "\n",
        "1. k-Fold Cross-Validation:\n",
        "\n",
        "The dataset is divided into k subsets (or \"folds\"). The model is trained on k-1 folds and tested on the remaining fold, and this process is repeated k times.\n",
        "\n",
        "The final result is the average accuracy across all k folds.\n",
        "\n",
        "\n",
        "\n",
        "2. Stratified k-Fold Cross-Validation:\n",
        "\n",
        "Similar to k-fold, but it ensures that the class distribution is approximately the same in each fold. This is especially useful when the data is imbalanced, ensuring that each fold has a representative distribution of classes.\n",
        "\n",
        "\n",
        "\n",
        "3. Leave-One-Out Cross-Validation (LOO-CV):\n",
        "\n",
        "In this extreme case, k is set to the total number of samples, so each test fold consists of a single sample, and the model is trained on the remaining data. This can be computationally expensive but is useful for small datasets.\n",
        "\n",
        "\n",
        "\n",
        "4. Repeated Cross-Validation:\n",
        "\n",
        "This involves performing k-fold cross-validation multiple times (with different random splits each time). Averaging the results across multiple runs gives even more consistent performance estimates.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Example: Cross-Validation Workflow\n",
        "\n",
        "Imagine you are training a model on a dataset with 100 samples. Using k-fold cross-validation (k=5):\n",
        "\n",
        "1. Split the data into 5 subsets.\n",
        "\n",
        "\n",
        "2. Train the model on 4 subsets (80 samples) and test it on the remaining 1 subset (20 samples).\n",
        "\n",
        "\n",
        "3. Repeat this process 5 times, each time using a different subset as the test set and the remaining ones as the training set.\n",
        "\n",
        "\n",
        "4. Compute the accuracy for each fold and average the accuracies.\n",
        "\n",
        "\n",
        "\n",
        "The final result is the average validation accuracy across the 5 splits, providing a more stable and reliable measure of the model’s performance compared to a single train-test split.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "When Does Averaging Help?\n",
        "\n",
        "Small Datasets: In small datasets, a single split might give highly variable results, so averaging helps provide a more accurate estimate.\n",
        "\n",
        "Imbalanced Datasets: If one class is underrepresented, a single split might lead to poor evaluation. Averaging across different splits (especially stratified cross-validation) ensures that all classes are represented adequately in both the training and testing phases.\n",
        "\n",
        "Model Tuning: When you tune hyperparameters, averaging validation accuracy across multiple splits gives you a better sense of the model's true performance, preventing overfitting to one particular data subset.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Limitations of Cross-Validation\n",
        "\n",
        "While cross-validation helps to reduce variance and provide more reliable results, it still has some limitations:\n",
        "\n",
        "1. Computational Cost: Cross-validation can be computationally expensive, especially for large datasets or complex models, because it requires training the model multiple times (one for each fold).\n",
        "\n",
        "\n",
        "2. Data Leakage: If data leakage occurs (for instance, when information from the test set is used during training), it can invalidate the cross-validation results.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Averaging validation accuracy across multiple splits in cross-validation does indeed provide more consistent and reliable results, especially in the presence of data variability, imbalanced classes, or small datasets. It helps mitigate the impact of random variations in the data and offers a better estimate of the model's generalization ability.\n",
        "\n",
        "Would you like to see how this works with an example or code implementation?\n",
        "\n",
        "\n",
        "2. Does it give more accurate estimate of test accuracy?\n",
        "Averaging validation accuracy across multiple splits (such as in cross-validation) generally provides a more reliable estimate of the model's performance, but it does not directly give a more accurate estimate of test accuracy. Here's why:\n",
        "\n",
        "Validation Accuracy vs. Test Accuracy\n",
        "\n",
        "Validation Accuracy: During cross-validation, the accuracy you compute is on the validation set (the held-out subset used for evaluation during training). This is used to estimate how well the model is likely to perform on new, unseen data.\n",
        "\n",
        "Test Accuracy: The test accuracy refers to the performance of the model on a completely separate test set that the model has never seen before (i.e., after training and validation are complete). This is the final metric you care about in terms of how well the model generalizes to real-world data.\n",
        "\n",
        "\n",
        "Why Cross-Validation Improves the Estimate of Test Accuracy:\n",
        "\n",
        "1. Reduces Overfitting to the Validation Set:\n",
        "Cross-validation helps reduce the risk of overfitting to a specific validation set by training the model multiple times on different subsets of the data. This makes the evaluation less dependent on the choice of a single validation split.\n",
        "\n",
        "\n",
        "2. Provides a More Generalizable Estimate:\n",
        "By averaging the performance across multiple validation splits, you are getting a more robust estimate of how the model might perform on new data, as it has been trained on different parts of the dataset each time. This makes the results more reliable than a single validation score.\n",
        "\n",
        "\n",
        "3. Mimics Real-World Testing:\n",
        "Since each fold in cross-validation is a different subset of the data, the averaged validation accuracy gives a better idea of how the model will generalize across different data, similar to how it will perform on real-world, unseen test data. This is more indicative of the model's generalization ability than a single split.\n",
        "\n",
        "\n",
        "\n",
        "Does Cross-Validation Give an Accurate Test Accuracy Estimate?\n",
        "\n",
        "No Direct Substitution for Test Accuracy:\n",
        "Cross-validation gives a good estimate of how well the model will perform, but it does not directly estimate test accuracy. The actual test accuracy, obtained from a separate test set that the model has never seen before, is the true measure of generalization.\n",
        "\n",
        "Overfitting Concern:\n",
        "While cross-validation helps reduce overfitting to the training set, there is still a potential for overfitting the model to the validation set used during cross-validation. The test accuracy will give the final verdict on the model’s generalization performance, which is the most reliable indicator.\n",
        "\n",
        "\n",
        "How Accurate is the Validation Accuracy Estimate?\n",
        "\n",
        "1. When Cross-Validation is Done Right:\n",
        "If you're using k-fold cross-validation (say, k=5 or k=10), the result is usually a very good approximation of test accuracy because:\n",
        "\n",
        "The model has been validated on different subsets of data.\n",
        "\n",
        "The model is likely to encounter similar data distributions in real-world use, so the validation accuracy reflects this.\n",
        "\n",
        "\n",
        "\n",
        "2. Bias and Variance in Cross-Validation:\n",
        "The mean of cross-validation scores is typically a good indicator of how the model will perform, but it can still suffer from:\n",
        "\n",
        "Bias: If your model is underfitting or overfitting, cross-validation might not fully capture that bias.\n",
        "\n",
        "Variance: If the data has a lot of variability, or if the dataset is very small, cross-validation results might still show some variance.\n",
        "\n",
        "\n",
        "\n",
        "3. Estimation of Test Accuracy:\n",
        "Cross-validation does a better job of estimating test accuracy than using a single train/test split, but it is still an estimate. The true test accuracy depends on how well the final model generalizes to the specific test set that is not involved in any part of the cross-validation process.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Can Cross-Validation Be Misleading?\n",
        "\n",
        "Yes, cross-validation could give an inaccurate estimate of test accuracy in certain situations:\n",
        "\n",
        "If the model has already been tuned on the validation data during the cross-validation process, you might get overly optimistic validation results, and these might not match the test accuracy.\n",
        "\n",
        "If the dataset is small and the splits do not adequately represent the variability in the data, cross-validation results could be more variable.\n",
        "\n",
        "If there is data leakage: If information from the test set has influenced the training process (even indirectly), cross-validation results could give an inflated estimate of the model's generalization ability.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "How to Get the Best Estimate of Test Accuracy?\n",
        "\n",
        "Use Cross-Validation During Model Selection:\n",
        "Cross-validation is particularly useful when you're selecting models or tuning hyperparameters. It helps you choose the model that generalizes the best, without overfitting to a particular train/test split.\n",
        "\n",
        "Hold Out a Final Test Set:\n",
        "After model selection (using cross-validation for training and validation), it's essential to test the final model on a separate test set that was not used in the cross-validation or any part of the training process. This final test set gives the true test accuracy.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Conclusion\n",
        "\n",
        "While cross-validation provides a more reliable estimate of test accuracy compared to a single train/test split, it is still just an approximation. The real test accuracy can only be obtained after training the model and evaluating it on a completely separate, unseen test set.\n",
        "\n",
        "Cross-validation helps by reducing the variance of the performance estimate, providing a better approximation of how the model will perform in practice. However, to obtain the true generalization performance, you still need to test the model on a separate test set after the cross-validation phase.\n",
        "\n",
        "\n",
        "\n",
        "3. What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "The number of iterations (or folds in cross-validation, or repeats in repeated cross-validation) can influence the reliability and stability of the performance estimate, but the effect depends on the context and how it's applied. Here's a detailed breakdown of the effect of increasing iterations on the estimate of model performance:\n",
        "\n",
        "1. Effect of Increasing Number of Folds in Cross-Validation (e.g., k-Fold Cross-Validation)\n",
        "\n",
        "In k-fold cross-validation, the number of iterations refers to the number of folds (k). Typically, k = 5 or k = 10 is used, but increasing or decreasing k changes the nature of the estimation.\n",
        "\n",
        "Benefits of Increasing k (More Folds):\n",
        "\n",
        "More Training Data: When you increase the number of folds (e.g., from k=5 to k=10), each fold gets a slightly smaller validation set, but each training set is larger. This can be helpful because the model has access to more data during training, which can help in improving generalization.\n",
        "\n",
        "Better Estimate of Generalization: Larger k values (such as k=10) reduce the variance of the validation accuracy estimate by training the model on a larger portion of the data for each validation, thus making the estimate more stable and reliable. With more folds, you get more training data for each fold, leading to potentially better model performance on unseen data.\n",
        "\n",
        "Lower Bias: Using more folds reduces the bias associated with the choice of validation set. A single fold might not be representative of the entire dataset, but with more folds, the model is validated across a broader spectrum of the data, resulting in a better approximation of how it will perform on unseen data.\n",
        "\n",
        "\n",
        "Downsides of Increasing k (More Folds):\n",
        "\n",
        "Computational Cost: Increasing the number of folds (k) also increases the computational cost because you have to train the model more times. For example, with k=5, the model is trained 5 times; with k=10, the model is trained 10 times.\n",
        "\n",
        "Diminishing Returns: After a certain point (usually around k=10), increasing the number of folds yields diminishing returns in terms of improved estimate stability. Larger k values only marginally improve the estimate, and the computational cost might not justify the small improvement.\n",
        "\n",
        "\n",
        "Optimal Number of Folds:\n",
        "\n",
        "Generally, k=5 or k=10 is a good balance between training time and stability of the estimate. In practice, increasing the number of folds beyond 10 does not drastically improve the results, but it can be useful if you have a smaller dataset and need a more accurate estimate of performance.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "2. Effect of Increasing the Number of Repeats (in Repeated Cross-Validation)\n",
        "\n",
        "In repeated cross-validation, the model is trained and validated multiple times, with each repeat using different random splits of the data.\n",
        "\n",
        "Benefits of Increasing the Number of Repeats:\n",
        "\n",
        "More Accurate Estimate: Repeating the cross-validation process multiple times helps reduce the variance in the performance estimate, especially when the dataset is small or highly variable. Averaging over many repetitions provides a more reliable estimate of the model's generalization ability.\n",
        "\n",
        "More Robust Evaluation: If a single run of cross-validation gives an unusually high or low score due to the random train-test split, repeating the cross-validation process helps smooth out these fluctuations, leading to more consistent and stable results.\n",
        "\n",
        "\n",
        "Downsides of Increasing the Number of Repeats:\n",
        "\n",
        "Increased Computation: The main downside of increasing the number of repeats is that it significantly increases the computational cost because you're running the cross-validation process multiple times, each with different splits of the data.\n",
        "\n",
        "Diminishing Returns: After a certain number of repeats (e.g., 5-10 repetitions), you start to see diminishing returns in terms of accuracy improvement. If the variance in the model's performance is low across the first few repeats, adding more repeats will provide less additional insight into the model's performance.\n",
        "\n",
        "\n",
        "Optimal Number of Repeats:\n",
        "\n",
        "A typical choice for repeated cross-validation is to repeat the cross-validation process 3-10 times, depending on the dataset size and the computational resources available. Increasing beyond this is usually unnecessary unless you are working with a very small or highly noisy dataset.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "3. Effect of Number of Iterations on Performance Estimates\n",
        "\n",
        "Improved Stability:\n",
        "\n",
        "Increasing the number of iterations (whether by increasing folds or repeats) typically results in a more stable and reliable estimate of the model's performance. The more iterations you run, the more representative your validation performance becomes because the model is tested across a wider variety of data splits.\n",
        "\n",
        "Lower Variance:\n",
        "\n",
        "If you use fewer iterations (such as with a single train-test split), the model evaluation could be highly variable based on the specific subset of data used for testing. With more iterations, the effect of any individual split is minimized, leading to more consistent results.\n",
        "\n",
        "\n",
        "Better Reflection of Generalization:\n",
        "\n",
        "More iterations mean that the model is trained on different data subsets, making the validation performance a better estimate of how the model will perform in real-world situations, on new unseen data.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "When Does More Iterations Not Help?\n",
        "\n",
        "Sufficient Data: If your dataset is very large, a smaller number of iterations (such as 5-fold cross-validation) may already provide a reliable estimate, and additional folds or repeats may yield diminishing returns. The model already has access to plenty of diverse training data, and the variance in performance will likely be low.\n",
        "\n",
        "Computational Constraints: More iterations require more computational resources. If you’re working with large datasets or complex models, running more iterations might be computationally prohibitive, and the additional gains in estimate accuracy might not justify the cost.\n",
        "\n",
        "Well-Defined Model and Data: If your model and data are relatively simple, and you have a good understanding of their behavior, you might not need a large number of iterations to get an accurate estimate.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Yes, more iterations (either through more folds or repeats) generally lead to a more consistent and reliable estimate of model performance, especially when there is variability in the data or when you have a small dataset.\n",
        "\n",
        "However, after a certain point, the benefit of additional iterations becomes marginal and the computational cost increases.\n",
        "\n",
        "k=5 or k=10 folds for cross-validation, or 3-10 repeats in repeated cross-validation, are often sufficient to get a good estimate of the model's generalization ability without significant diminishing returns.\n",
        "\n",
        "\n",
        "If you want a more precise estimate and are willing to invest the computational time, increasing the number of iterations (folds or repeats) can help, but for most practical applications, a reasonable number of iterations should be enough to get reliable performance estimates.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4. Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?\n",
        "Yes, increasing the number of iterations can help deal with a very small training or validation dataset, but it's not a perfect solution. Let's break down how it can help and what limitations exist when using small datasets:\n",
        "\n",
        "How Increasing Iterations Can Help with Small Datasets:\n",
        "\n",
        "1. Better Use of Available Data:\n",
        "\n",
        "With small datasets, a single train-test split might not provide a representative sample of the data, which can lead to high variance in the performance estimate. By increasing the number of iterations (e.g., through more folds in cross-validation or more repeats in repeated cross-validation), you ensure that the model is tested on different subsets of the data, which helps to maximize the use of the available data and improve the robustness of the performance estimate.\n",
        "\n",
        "\n",
        "\n",
        "2. Stabilizing Performance Estimates:\n",
        "\n",
        "For a small dataset, any single test split could be unrepresentative of the overall data, leading to misleading evaluation metrics. Increasing the number of iterations helps to average out any bias or variance caused by random splits, resulting in more stable and consistent estimates of the model's performance.\n",
        "\n",
        "Repeated cross-validation (multiple repetitions of k-fold cross-validation) can be particularly useful, as it ensures that the model has been tested on different random partitions of the data, providing a more reliable estimate of performance across a small sample.\n",
        "\n",
        "\n",
        "\n",
        "3. Reducing the Risk of Overfitting:\n",
        "\n",
        "With very small training sets, models can easily overfit the data. By using multiple splits, the model is exposed to different training and validation sets, which can help reduce overfitting to specific examples in the dataset. This improves the generalization of the model, as it's being trained on different subsets of data and evaluated across them.\n",
        "\n",
        "\n",
        "\n",
        "4. Stratified Cross-Validation:\n",
        "\n",
        "In cases of small datasets, especially with imbalanced classes, stratified cross-validation is useful. It ensures that each fold in cross-validation has the same proportion of each class as the entire dataset. This prevents situations where a fold might contain an unbalanced sample of classes, which could bias the results.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Limitations of Using More Iterations on Small Datasets:\n",
        "\n",
        "1. Increased Computational Cost:\n",
        "\n",
        "While increasing the number of folds or repeats helps stabilize performance estimates, it also comes with the downside of increased computation. With very small datasets, running many iterations can become computationally expensive without providing substantial improvements in the reliability of the estimate.\n",
        "\n",
        "\n",
        "\n",
        "2. Diminishing Returns:\n",
        "\n",
        "If the dataset is extremely small, even with more folds or repeats, the variance in the results may still be high because you're still working with limited data. Adding more folds (e.g., going from k=5 to k=10) may not significantly improve the stability of the estimate, and you might reach a point of diminishing returns.\n",
        "\n",
        "\n",
        "\n",
        "3. Overfitting to Small Data:\n",
        "\n",
        "Even with multiple folds or repetitions, a very small dataset might still result in overfitting if the model has too many parameters or is too complex relative to the size of the dataset. In these cases, even cross-validation won't be able to prevent the model from memorizing the few examples it has access to, leading to poor generalization.\n",
        "\n",
        "\n",
        "\n",
        "4. Limited Representation of the Data:\n",
        "\n",
        "With extremely small datasets, there's always the issue that even with many iterations, the folds or repeats may still not capture the full diversity of the data. The small number of examples might lead to results that aren't fully reflective of how the model will perform on truly unseen data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Alternative Solutions for Small Datasets\n",
        "\n",
        "While increasing the number of iterations helps with small datasets, here are some other strategies that can improve model performance and estimation reliability when data is limited:\n",
        "\n",
        "1. Data Augmentation:\n",
        "\n",
        "For certain types of data (especially images, text, and time-series), you can use data augmentation techniques to artificially increase the size of the dataset by generating new, slightly modified examples. This can help the model generalize better and reduce overfitting.\n",
        "\n",
        "\n",
        "\n",
        "2. Transfer Learning:\n",
        "\n",
        "If you are working with very small datasets, transfer learning can be highly effective. It involves using a model that has been pre-trained on a larger dataset and fine-tuning it on your small dataset. This approach leverages knowledge from a larger corpus and helps improve generalization without requiring a large dataset.\n",
        "\n",
        "\n",
        "\n",
        "3. Regularization Techniques:\n",
        "\n",
        "When working with small datasets, it's crucial to use regularization techniques (like L1/L2 regularization, dropout, etc.) to prevent overfitting. Regularization can help constrain the model's complexity, making it more robust when working with limited data.\n",
        "\n",
        "\n",
        "\n",
        "4. Synthetic Data Generation:\n",
        "\n",
        "If augmentation techniques are not possible, consider using methods like SMOTE (Synthetic Minority Over-sampling Technique) to generate synthetic examples, especially in cases of imbalanced datasets.\n",
        "\n",
        "\n",
        "\n",
        "5. Use of Simpler Models:\n",
        "\n",
        "Complex models with many parameters (e.g., deep neural networks) require a lot of data to generalize well. With small datasets, it might be better to use simpler models (like linear regression, decision trees, or small random forests) that are less likely to overfit.\n",
        "\n",
        "\n",
        "\n",
        "6. Bootstrapping:\n",
        "\n",
        "Bootstrapping is another resampling technique that can help in cases of small datasets. It involves creating multiple subsets of the data by sampling with replacement and training the model on those subsets. This is especially useful for estimating model uncertainty when data is limited.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Conclusion:\n",
        "\n",
        "Yes, increasing the number of iterations (whether through more folds in cross-validation or more repeats) can help deal with a small training or validation dataset by:\n",
        "\n",
        "Providing more stable performance estimates,\n",
        "\n",
        "Maximizing the use of limited data,\n",
        "\n",
        "Reducing overfitting risks.\n",
        "\n",
        "\n",
        "However, the benefits have diminishing returns as the dataset size becomes extremely small, and it’s still important to consider other strategies like data augmentation, transfer learning, regularization, or simpler models to improve performance and generalization. If you have very limited data, relying solely on increasing iterations might not fully solve the problem, but it will still provide more reliable estimates compared to using a single train-test split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-SBxy1qevgJ"
      },
      "source": [
        "> Exercise: How does the accuracy of the 3 nearest neighbour classifier change with the number of splits? How is it affected by the split size? Compare the results with the 1 nearest neighbour classifier."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}